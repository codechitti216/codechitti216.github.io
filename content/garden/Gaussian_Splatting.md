---
title: "NeRFs and Gaussian Splatting"
date: "2025-08-05"
tags: ["NeRF", "Gaussian Splatting", "View Synthesis", "Rendering", "MLP", "3D Perception"]
status: "ongoing"
kind: "learning"
published: true
visibility: "public"
evolution:
  - date: "2025-08-05"
    note: "Initial curiosity about rendering and Gaussian Splatting."
  - date: "2025-08-05"
    note: "Learned what NeRF is and how it uses volume rendering and MLPs."
  - date: "2025-08-05"
    note: "Understood Gaussian Splatting as explicit, differentiable geometry proxy."
  - date: "2025-08-05"
    note: "Refined understanding of inference, initialization, and learned geometry."
  - date: "2025-08-05"
    note: "Confirmed Gaussian Splatting as a train-once, render-fast paradigm."
---


# üß† A Journal of Evolving Intuition: NeRFs and Gaussian Splatting

### 2025 - 08 - 05

## üå± Starting Point

**Prompted by curiosity**, I asked: ‚ÄúWhat is Gaussian Splatting?‚Äù All I knew was that it was some rendering method involving point clouds.

That kicked off the cascade.

I realized that to understand Gaussian Splatting, I first needed to understand NeRF ‚Äî which I barely knew existed.

I started from ground zero:

* ‚ÄúWhat is rendering?‚Äù
* ‚ÄúIsn‚Äôt it like meshing ‚Äî turning raw data into something visual?‚Äù
* ‚ÄúWhat‚Äôs this ‚Äòview direction‚Äô? And how does radiance relate to pixels?‚Äù

## üîç Encountering NeRF

What I first learned: NeRF is **not a geometry model**, but a **neural function** that maps 3D coordinates and viewing directions to color and density.

### Early Realizations:

* ‚ùå Thought radiance and density were grayscale-like.
  ‚úÖ Learned radiance is *emitted color in a view-dependent way*, and density governs transmittance.
* ‚ùå Thought each pixel had its own function or model.
  ‚úÖ Realized it‚Äôs one MLP shared across the scene.
* ‚úÖ Understood rays: camera ‚Üí pixel ‚Üí cast ray ‚Üí sample 3D points.

### Pipeline Clicked:

> Sample 3D points on a ray ‚Üí pass (x, d) into MLP ‚Üí integrate color + transmittance ‚Üí predict pixel RGB ‚Üí compare to ground truth ‚Üí backprop through the whole ray.

My mental reframe:

> ‚ÄúNeRF is a learned, differentiable form of volumetric projection.‚Äù

### Training Efficiency:

* ‚ùå Thought of batches as one-ray-per-batch.
  ‚úÖ Understood minibatches of many rays, sampled randomly, for efficiency.

## ü§Ø Turning to Gaussian Splatting

Then came the shift:

> ‚ÄúIf NeRF learns a function, what does Gaussian Splatting do?‚Äù

### First Analogies:

> ‚ÄúIt‚Äôs like stacking transparent sheets (Gaussians) and projecting them.‚Äù
> ‚ÄúIt‚Äôs like splattering colored paintballs on a canvas and looking from behind.‚Äù

### Core Understanding:

* Starts with a **sparse point cloud** (e.g. from COLMAP)
* Initializes **one Gaussian per point**
* Each Gaussian has:

  * Position (Œº)
  * Covariance (Œ£)
  * Color
  * Opacity
  * View-dependent features (e.g. Spherical Harmonics)
* All of these are **trainable parameters**
* Rendering = **project and alpha-blend** Gaussians
* Backpropagate image loss to update Gaussian parameters ‚Äî including positions!

### Corrected Assumptions:

* ‚ùå Thought the point cloud stayed fixed.
  ‚úÖ Even Gaussian *centers* (positions) are optimized.
* ‚ùå Thought inference required re-initializing Gaussians.
  ‚úÖ Training is one-time; inference is just projection.

> ‚ÄúAfter training, you discard the original point cloud ‚Äî the optimized Gaussians *are* your scene.‚Äù

## üß† From Function to Proxy

|                | NeRF                          | Gaussian Splatting                      |
| -------------- | ----------------------------- | --------------------------------------- |
| Representation | Implicit (MLP function)       | Explicit (trainable 3D Gaussians)       |
| Geometry       | Not directly accessible       | Encoded in positions, shapes of blobs   |
| Rendering      | Sample & integrate along rays | Project & blend Gaussians               |
| Training Data  | Only multi-view images        | Multi-view images + initial point cloud |
| Inference Time | Slow (unless optimized)       | Real-time                               |
| Editable       | Hard (function entangled)     | Easy (Gaussians are modular)            |

> ‚ÄúNeRF learns how to simulate light transport.‚Äù
> ‚ÄúGaussian Splatting learns to be seen.‚Äù

## üîé Diagnostic-Level Intuition

> ‚ÄúAfter training, we have a ‚Äòmini Gaussian world‚Äô that, when projected from the 50 known views, reconstructs the original images near-perfectly.‚Äù ‚úÖ Yes.

> ‚ÄúAt inference, we use the trained Gaussians ‚Äî not the raw point cloud.‚Äù ‚úÖ Yes.

> ‚ÄúGaussian Splatting is not about learning a rendering function ‚Äî it *is* the rendering structure.‚Äù ‚úÖ Yes.

## üß≠ What‚Äôs Next

### Still Curious About:

* How view-dependent shading is encoded via Spherical Harmonics
* How occlusion is handled explicitly in splatting vs via density in NeRF
* What happens when GS is trained on extremely sparse data
* How to compress or prune Gaussians for edge devices
* How dynamic scenes can be represented in GS (temporal Gaussians?)

### What‚Äôs Solidified:

* ‚úÖ Volume rendering via NeRF‚Äôs sampling ‚Üí function ‚Üí integration pipeline
* ‚úÖ Differentiable splatting as a projection-first, geometry-aware approach
* ‚úÖ Trade-offs: NeRF is expressive but slow; GS is fast and modular

---

## ‚úÖ What I'm Studying

How two different paradigms ‚Äî **NeRFs** and **Gaussian Splatting** ‚Äî enable view synthesis from sparse inputs. One learns a light-emitting function, the other builds a proxy geometry of 3D blobs.

---

## üéØ My Model of NeRF (Neural Radiance Fields)

### Summary

NeRF is a **function approximator (MLP)** that learns:

$$
F_\theta(x \in \mathbb{R}^3, \, d \in \mathbb{R}^3) \rightarrow (r, g, b, \sigma)
$$

where $x$ is a 3D position, $d$ is a viewing direction, and the output is view-dependent color + density.

One MLP for the entire scene.

### Training:

* Input = posed RGB images (camera intrinsics + extrinsics)
* For each pixel:

  * Cast a ray
  * Sample N points along the ray
  * Query the MLP at each point
  * Volume render the RGB
  * Compare to ground truth pixel
  * Backpropagate across all rays

> "NeRF doesn‚Äôt build geometry ‚Äî it learns how light behaves in 3D space."

It‚Äôs an **implicit model** ‚Äî the scene is encoded in the weights.

### What I Got Wrong (Initially):

* Thought MLPs were per-pixel ‚Äî false. One global MLP.
* Misunderstood volume rendering ‚Äî now clear it's the integration of light along a ray.
* Didn‚Äôt get why multiple samples are needed ‚Äî now I see it‚Äôs to handle occlusion and variation.

---

## ‚úÖ My Model of Gaussian Splatting

### Summary

GS is a **proxy-based renderer** using **3D Gaussian ellipsoids**.

Each Gaussian $\mathcal{G}_i$ has:

* Position $\mu_i$
* Covariance $\Sigma_i$
* Color (RGB or Spherical Harmonics)
* Opacity + visibility weights

### Training:

* Input = point cloud + posed RGB images
* Initialize one Gaussian per point
* Render all Gaussians per view
* Backpropagate loss w\.r.t. all Gaussian attributes

### Inference:

* Freeze Gaussians
* For any novel view:

  * Project Gaussians
  * Blend using alpha composition

> "Rendering = projection, not sampling. No MLP. No integration."

---

## üîÅ Core Contrast

| Aspect              | NeRF                       | Gaussian Splatting           |
| ------------------- | -------------------------- | ---------------------------- |
| Representation      | Implicit function via MLP  | Explicit proxy via Gaussians |
| Geometry            | Encoded in weights         | Encoded in positions/shapes  |
| Rendering           | Ray sampling + integration | Projection + alpha blending  |
| Training Inputs     | Posed images               | Images + point cloud         |
| Inference Time      | Slow (unless optimized)    | Real-time                    |
| Editable Components | Hard                       | Modular + intuitive          |

---
### 2025 - 08 - 06
---
