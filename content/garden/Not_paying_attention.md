---
title: "Paying Attention in the Wrong Place?"
date: "2025-07-28"
tags: ["Machine Learning", "Learning Dynamics", "Ideas", "Thoughts"]
status: "evolving"
---

# Where is attention usually used? 

In most of the architectures, attention is usually used after the feature extraction after the encoder to make the features rich or in the decoder to understand the output at a more deeper level. 

Chris Evert, one of the most successful tennis legends of all time said.. /"“To me, consistency is the essence of tennis technique. It’s not about hitting winners—it’s about making fewer mistakes.”"/

But in reality, a person performs the best when his entire focus is completely on avoiding the errors.. 

I hypothesis that adding a small attention layer at the loss calculation would be a really helpful to the training process and will make it more fast, effective and efficient

This is still a hunch.. I need to work on this.. formulate it, design the experimental setup.. 
